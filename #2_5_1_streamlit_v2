

# =========================================================
# Streamlit ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ RAG ì±—ë´‡ v5
# 
# ê°œì„ ì‚¬í•­:
# 1. Validate ê¸°ë°˜ íšŒë³µ ë£¨í”„ (PASS/FAIL ë¶„ê¸°)
# 2. Clarify ëŒ€í™”í˜• ì¤‘ë‹¨ + ìƒíƒœ ì €ì¥
# 3. Query Rewrite / Rerank ë…¸ë“œ
# 4. ì•ˆì „ ê°€ë“œ ë…¸ë“œ (Context Sanitize, Safety Check)
# =========================================================
import streamlit as st
import json
import re
import os
import pandas as pd
from typing import Dict, Any, List, Optional, Literal, TypedDict

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# =========================================================
# í˜ì´ì§€ ì„¤ì •
# =========================================================
st.set_page_config(
    page_title="ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ì±—ë´‡ v5",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =========================================================
# ì»¤ìŠ¤í…€ CSS
# =========================================================
st.markdown("""
<style>
    .main .block-container {
        padding-top: 1rem;
        padding-bottom: 2rem;
    }
    
    .guide-box {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1.2rem;
        border-radius: 12px;
        margin-bottom: 1.5rem;
        box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
    }
    
    .guide-title {
        font-size: 1.1rem;
        font-weight: 700;
        margin-bottom: 0.8rem;
        display: flex;
        align-items: center;
        gap: 0.5rem;
    }
    
    .guide-item {
        background: rgba(255,255,255,0.15);
        padding: 0.7rem 1rem;
        border-radius: 8px;
        margin-bottom: 0.5rem;
        font-size: 0.9rem;
        line-height: 1.5;
    }
    
    .guide-item:last-child {
        margin-bottom: 0;
    }
    
    .status-box {
        background-color: #e3f2fd;
        padding: 0.8rem 1rem;
        border-radius: 8px;
        border-left: 4px solid #2196f3;
        margin: 0.5rem 0;
        font-weight: 500;
    }
    
    .retry-badge {
        background-color: #fff3e0;
        color: #e65100;
        padding: 0.2rem 0.5rem;
        border-radius: 4px;
        font-size: 0.8rem;
        font-weight: 600;
    }
    
    .validation-pass {
        color: #2e7d32;
        font-weight: 600;
    }
    
    .validation-fail {
        color: #c62828;
        font-weight: 600;
    }
    
    h1 {
        color: #1a237e;
    }
</style>
""", unsafe_allow_html=True)

# =========================================================
# ìƒìˆ˜ ì„¤ì •
# =========================================================
YEAR_TO_FILENAME = {
    2020: "2020ë…„_ìŠ¤ë§ˆíŠ¸í°_ê³¼ì˜ì¡´_ì‹¤íƒœì¡°_ì‚¬ë³´ê³ ì„œ.pdf",
    2021: "2021ë…„_ìŠ¤ë§ˆíŠ¸_ê³¼ì˜ì¡´_ì‹¤íƒœì¡°ì‚¬_ë³´ê³ ì„œ.pdf",
    2022: "2022ë…„_ìŠ¤ë§ˆíŠ¸í°_ê³¼ì˜ì¡´_ì‹¤íƒœì¡°ì‚¬_ë³´ê³ ì„œ.pdf",
    2023: "2023ë…„_ìŠ¤ë§ˆíŠ¸í°_ê³¼ì˜ì¡´ì‹¤íƒœì¡°ì‚¬_ìµœì¢…ë³´ê³ ì„œ.pdf",
    2024: "2024_ìŠ¤ë§ˆíŠ¸í°_ê³¼ì˜ì¡´_ì‹¤íƒœì¡°ì‚¬_ë³¸_ë³´ê³ ì„œ.pdf",
}
ALLOWED_FILES = list(YEAR_TO_FILENAME.values())

BOT_IDENTITY = """2020~2024ë…„ ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

**ì œê³µ ê°€ëŠ¥í•œ ì •ë³´:**
- ì—°ë„ë³„ ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ìœ„í—˜êµ° ë¹„ìœ¨ ë° ì¶”ì´
- ëŒ€ìƒë³„(ìœ ì•„ë™, ì²­ì†Œë…„, ì„±ì¸, 60ëŒ€) ê³¼ì˜ì¡´ í˜„í™©
- í•™ë ¹ë³„(ì´ˆ/ì¤‘/ê³ /ëŒ€í•™ìƒ) ì„¸ë¶€ ë¶„ì„
- ê³¼ì˜ì¡´ ê´€ë ¨ ìš”ì¸ ë¶„ì„ (SNS, ìˆí¼, ê²Œì„ ì´ìš© ë“±)
- ì¡°ì‚¬ ë°©ë²•ë¡  ë° í‘œë³¸ ì„¤ê³„ ì •ë³´
"""

# Hugging Face ì„¤ì •
HF_REPO_ID = "Rosaldowithbaek/smartphone-addiction-chroma-db"
LOCAL_DB_PATH = "./chroma_db_store"

# ê²€ìƒ‰ íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’ / ì¬ì‹œë„ìš©)
DEFAULT_K_PER_QUERY = 10
DEFAULT_TOP_PARENTS = 15
DEFAULT_TOP_PARENTS_PER_FILE = 5

RETRY_K_PER_QUERY = 15
RETRY_TOP_PARENTS = 20
RETRY_TOP_PARENTS_PER_FILE = 7

MAX_CHUNKS_PER_PARENT = 5
MAX_CHARS_PER_DOC = 10000
SUMMARY_TYPES = ["page_summary", "table_summary"]

MAX_RETRY_COUNT = 2

# í‚¤ì›Œë“œ ë¶„ë¥˜
TARGET_KEYWORDS = {
    # ê¸°ì¡´ í‚¤ ìœ ì§€í•˜ë©´ì„œ, ë³´ê³ ì„œ í‘œí˜„/ì—°ë ¹ í‘œê¸°/ë³€í˜•ì„ ìµœëŒ€í•œ í¡ìˆ˜í•¨
    "ëŒ€ìƒ": [
        # ì „ì²´/ëª¨ì§‘ë‹¨ í‘œí˜„
        "ì „ì²´", "ì „êµ­", "ì „êµ­ë¯¼", "ì „êµ­ ê°€êµ¬", "ëª¨ì§‘ë‹¨", "ì¡°ì‚¬ëŒ€ìƒ", "ì¡°ì‚¬ ëª¨ì§‘ë‹¨",
        "ìŠ¤ë§ˆíŠ¸í° ì´ìš©ì", "ìŠ¤ë§ˆíŠ¸í°(ì¸í„°ë„·) ì´ìš©ì", "ì´ìš©ì",

        # ë³´ê³ ì„œì—ì„œ ì“°ëŠ” ëŒ€ìƒ êµ¬ë¶„(ì—°ë ¹ëŒ€ í° ë©ì–´ë¦¬)
        "ìœ ì•„ë™", "ì˜ìœ ì•„", "ìœ ì•„", "ì•„ë™", "ì–´ë¦°ì´", "ë§Œ 3~9ì„¸", "ë§Œ3~9ì„¸", "ë§Œ 3âˆ¼9ì„¸",
        "ì²­ì†Œë…„", "10ëŒ€", "ì‹­ëŒ€", "10 ëŒ€", "ë§Œ 10~19ì„¸", "ë§Œ10~19ì„¸", "ë§Œ 10âˆ¼19ì„¸",
        "ì„±ì¸", "ë§Œ 20~59ì„¸", "ë§Œ20~59ì„¸", "ë§Œ 20âˆ¼59ì„¸",
        "60ëŒ€", "ê³ ë ¹ì¸µ", "ê³ ë ¹ì", "ë§Œ 60~69ì„¸", "ë§Œ60~69ì„¸", "ë§Œ 60âˆ¼69ì„¸",
    ],
    "ìœ„í—˜êµ°": [    # ë³´ê³ ì„œ ë¶„ë¥˜ ì²´ê³„(ê³ ìœ„í—˜/ì ì¬ì /ì¼ë°˜ + ê³¼ì˜ì¡´ìœ„í—˜êµ°=ê³ ìœ„í—˜+ì ì¬ì )
    "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´", "ê³¼ì˜ì¡´", "ê³¼ë‹¤ì´ìš©",
    "ê³¼ì˜ì¡´ ìˆ˜ì¤€", "ê³¼ì˜ì¡´ ìˆ˜ì¤€ë³„",
    "ê³¼ì˜ì¡´ìœ„í—˜êµ°", "ê³¼ì˜ì¡´ ìœ„í—˜êµ°", "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ìœ„í—˜êµ°", "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ìœ„í—˜êµ°",
    "ê³ ìœ„í—˜êµ°", "ê³  ìœ„í—˜êµ°",
    "ì ì¬ì ìœ„í—˜êµ°", "ì ì¬ì  ìœ„í—˜êµ°", "ì ì¬ ìœ„í—˜êµ°",
    "ì¼ë°˜ì‚¬ìš©ìêµ°", "ì¼ë°˜ ì‚¬ìš©ìêµ°", "ì¼ë°˜êµ°",
    ],

    "í•™ë ¹": [
        # ê¸°ì¡´ + ë³€í˜•/ë™ì˜ í‘œí˜„
        "ìœ ì¹˜ì›ìƒ", "ìœ ì¹˜ì›", "ë¯¸ì·¨í•™", "ë¯¸ì·¨í•™ ì•„ë™",
        "ì´ˆë“±í•™ìƒ", "ì´ˆë“±", "ì´ˆë“±ìƒ", "ì´ˆë“±í•™êµ", "ì´ˆë“± ì €í•™ë…„", "ì´ˆë“± ê³ í•™ë…„",
        "ì¤‘í•™ìƒ", "ì¤‘ë“±", "ì¤‘í•™êµ", "ì¤‘ë“±í•™ìƒ",
        "ê³ ë“±í•™ìƒ", "ê³ ë“±", "ê³ ë“±í•™êµ", "ê³ ë“±ìƒ",
        "ëŒ€í•™ìƒ", "ëŒ€í•™", "ëŒ€í•™êµ", "ëŒ€í•™ ì¬í•™ìƒ",
    ],

    "ì„±ë³„": [
        "ë‚¨ì„±", "ì—¬ì„±", "ë‚¨ì", "ì—¬ì",
        "ë‚¨", "ì—¬", "ë‚¨ë…€", "ì„±ë³„",
    ],
    "ì§€ì—­": ["ëŒ€ë„ì‹œ", "ì¤‘ì†Œë„ì‹œ", "ìë©´ì§€ì—­", "ì/ë©´"],
    "ìœ„í—˜êµ°": ["ê³¼ì˜ì¡´ìœ„í—˜êµ°", "ì¼ë°˜ì‚¬ìš©ìêµ°", "ê³ ìœ„í—˜êµ°", "ì ì¬ì ìœ„í—˜êµ°"],
}

TOPIC_KEYWORDS = {
    "ì½˜í…ì¸ ": [
        # (í•µì‹¬) ë³´ê³ ì„œ â€˜ì½˜í…ì¸  ì´ìš©ì •ë„â€™ 26ê°œ ë¶„ë¥˜ ê¸°ë°˜
        "SNS", "ì´ë©”ì¼", "ë©”ì‹ ì €", "ìƒˆë¡œìš´ ì¹œêµ¬ë§Œë‚¨", "ìƒˆë¡œìš´ ì¹œêµ¬ ë§Œë‚¨",
        "ìƒí™œê´€ë¦¬", "ê±´ê°•ê´€ë¦¬", "í™”ìƒíšŒì˜", "ì›ê²©ê·¼ë¬´", "í™”ìƒíšŒì˜/ì›ê²©ê·¼ë¬´",
        "ì‡¼í•‘", "ì‡¼í•‘(ìƒí’ˆ/ì„œë¹„ìŠ¤)", "ìƒí’ˆ/ì„œë¹„ìŠ¤ íŒë§¤", "ê¸ˆìœµê±°ë˜", "íˆ¬ì ë° ìì‚°ê´€ë¦¬",
        "ê²Œì„", "ì˜í™”/TV/ë™ì˜ìƒ", "ì˜í™”", "TV", "ë™ì˜ìƒ",
        "ìŒì•…", "ë¼ë””ì˜¤", "íŒŸìºìŠ¤íŠ¸", "ë¼ë””ì˜¤/íŒŸìºìŠ¤íŠ¸",
        "ì›¹íˆ°", "ì›¹ì†Œì„¤", "ë…ì„œ", "ì›¹íˆ°/ì›¹ì†Œì„¤/ë…ì„œ",
        "ì‚¬ì§„", "ì´¬ì˜", "í¸ì§‘", "ì‚¬ì§„(ì´¬ì˜ í¸ì§‘) ë° ê·¸ë¦¼", "ê·¸ë¦¼",
        "ì—¬í–‰",
        "ì„±ì¸ìš© ì½˜í…ì¸ ", "ì‚¬í–‰ì„± ê²Œì„",
        "ë‰´ìŠ¤ë³´ê¸°", "ë‰´ìŠ¤ ë³´ê¸°",
        "í•™ì—…/ì—…ë¬´ìš© ê²€ìƒ‰", "í•™ì—…", "ì—…ë¬´", "ì—…ë¬´ìš© ê²€ìƒ‰",
        "ê´€ì‹¬ì‚¬(ì·¨ë¯¸)ê²€ìƒ‰", "ì·¨ë¯¸", "ê´€ì‹¬ì‚¬ ê²€ìƒ‰",
        "ì§€ë„", "ë„¤ë¹„ê²Œì´ì…˜", "ì§€ë„ ë° ë„¤ë¹„ê²Œì´ì…˜",
        "êµìœ¡", "ì›ê²©ìˆ˜ì—…", "E-ëŸ¬ë‹", "ì¸í„°ë„·ê°•ì˜", "êµìœ¡(ì›ê²©ìˆ˜ì—…/E-ëŸ¬ë‹/ì¸í„°ë„·ê°•ì˜)",
        "ìƒì„±í˜• AIì„œë¹„ìŠ¤", "ìƒì„±í˜•AI", "ì •ë³´ê²€ìƒ‰", "ë¬¸ì„œë³´ì¡°", "ë²ˆì—­",

        # ì˜¨ë¼ì¸ ë™ì˜ìƒ ì„œë¹„ìŠ¤(OVS)Â·ìˆí¼ ê´€ë ¨(ë³´ê³ ì„œ í‘œ/ê·¸ë¦¼ í‘œí˜„ ë°˜ì˜)
        "ì˜¨ë¼ì¸ ë™ì˜ìƒ ì„œë¹„ìŠ¤", "ì˜¨ë¼ì¸ë™ì˜ìƒì„œë¹„ìŠ¤", "ë™ì˜ìƒ ì„œë¹„ìŠ¤",
        "ìˆí¼", "ì‡¼ì¸ ", "ë¦´ìŠ¤", "ìˆí¼ í”Œë«í¼",

        # ë³´ê³ ì„œì—ì„œ ì œì‹œëœ â€˜ì£¼ ì´ìš© ìˆí¼ í”Œë«í¼(1ìˆœìœ„)â€™ í•­ëª©(ê³ ìœ ëª…)
        "ìœ íŠœë¸Œ ì‡¼ì¸ ", "ì¸ìŠ¤íƒ€ê·¸ë¨ ë¦´ìŠ¤", "í‹±í†¡", "ì¹´ì¹´ì˜¤í†¡", "ë„¤ì´ë²„ í´ë¦½",

    ],

    "ì§€í‘œ": [
        # í•µì‹¬ ê²°ê³¼ ì§€í‘œ
        "ê³¼ì˜ì¡´ë¥ ", "ê³¼ì˜ì¡´ ìœ„í—˜êµ° ë¹„ìœ¨", "ê³¼ì˜ì¡´ìœ„í—˜êµ° ë¹„ìœ¨",
        "ê³ ìœ„í—˜êµ° ë¹„ìœ¨", "ì ì¬ì ìœ„í—˜êµ° ë¹„ìœ¨", "ì¼ë°˜ì‚¬ìš©ìêµ° ë¹„ìœ¨",
        "ë¹„ìœ¨", "ë¥ ", "%", "%p", "ë‹¨ìœ„:%", "ë‹¨ìœ„: %",

        # ì²™ë„/ì ìˆ˜
        "ì ìˆ˜", "ì´ì ", "í‰ê· ", "4ì  ë§Œì ", "4ì ë§Œì ",
        "ê¸°ì¤€ì ìˆ˜", "ê¸°ì¤€ ì ìˆ˜", "ì—­ë¬¸í•­", "ì—­ì²™ë„",

        # ë¹„êµ/ì¶”ì´ í‘œí˜„
        "ì—°ë„ë³„", "ì „ë…„ëŒ€ë¹„", "ìµœê·¼ 1ë…„", "ìµœê·¼1ë…„", "ì¶”ì´", "ì¦ê°€", "ê°ì†Œ", "ë³€í™”",
        "ëŒ€ìƒë³„", "ì—°ë ¹ë³„", "ì—°ë ¹ëŒ€ë³„", "ì„±ë³„", "í•™ë ¹ë³„", "ë„ì‹œê·œëª¨ë³„",

        # ê³¼ë‹¤ì´ìš©Â·ì¡°ì ˆ
        "ê³¼ë‹¤ì´ìš©", "ê³¼ë‹¤ì´ìš© ì¸ì‹",
        "ì´ìš©ì‹œê°„", "ì´ìš© ì‹œê°„", "ì´ìš©ì‹œê°„ ì¡°ì ˆ", "ì´ìš©ì‹œê°„ ì¡°ì ˆ ì–´ë ¤ì›€",
        "ë³¸ì¸ ì˜ì§€ëŒ€ë¡œ ì¡°ì ˆ", "ì¡°ì ˆ ì–´ë ¤ì›€ ì •ë„",
    ],

    "ìš”ì¸": [
        # ê³¼ì˜ì¡´ 3ìš”ì¸(ë³´ê³ ì„œ ê³µí†µ í”„ë ˆì„)
        "ì¡°ì ˆì‹¤íŒ¨", "í˜„ì €ì„±", "ë¬¸ì œì  ê²°ê³¼",

        # ì‹¬ì¸µë¬¸í•­/ê²½í—˜ ì˜ì—­(ëª©ì°¨Â·í‘œ ì œëª© ê¸°ë°˜)
        "ì‚¬ìš©ì¡°ì ˆ", "ì‚¬ìš© ì¡°ì ˆ",
        "ìƒí™œ ìš°ì„ ì„±", "ìŠ¤ë§ˆíŠ¸í°ì˜ ìƒí™œ ìš°ì„ ì„±",
        "íí•´ ê²½í—˜", "íí•´", "ë¶€ì •ì  ê²°ê³¼",
        "ì‹ ì²´ ê±´ê°•", "ì •ì‹  ê±´ê°•", "ëŒ€ì¸ê´€ê³„", "ëŒ€ì¸ê´€ê³„ ë§¥ë½", "ìƒì‚°ì„± ì €í•˜",

        # ìˆí¼ ê´€ë ¨ ì˜í–¥ìš”ì¸
        "ìˆí¼ ì‹œì²­ ì¡°ì ˆ", "ìˆí¼ ì‹œì²­ ì¡°ì ˆì˜ ì–´ë ¤ì›€", "ì•Œê³ ë¦¬ì¦˜", "ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜", "ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì¸í•œ ìˆí¼ ì‹œì²­ ì˜í–¥",

        # ìƒí™œÂ·ì—­ëŸ‰Â·ë§Œì¡±ë„(ë³´ê³ ì„œ êµ¬ì„±)
        "ì—¬ê°€í™œë™", "ì£¼ ì—¬ê°€í™œë™", "í¬ë§í•˜ëŠ” ì—¬ê°€í™œë™",
        "ë””ì§€í„¸ ì‚¬ìš© ì—­ëŸ‰", "ì •ë³´ ê²€ìƒ‰ ì—­ëŸ‰", "ì •ë³´ ì‹ ë¢° íŒë‹¨", "ì‚¬íšŒë¬¸ì œ ì°¸ì—¬", "ì½˜í…ì¸  ì œì‘/í¸ì§‘",
        "ê°œì¸ì •ë³´ë³´í˜¸", "í”„ë¼ì´ë²„ì‹œ", "í•™ì—…Â·ì§ì—… ê´€ë ¨ í™œë™",
        "ì‚¶ì˜ ë§Œì¡±ë„", "ì „ë°˜ì  ë§Œì¡±ë„", "ì¸ê°„ê´€ê³„ ë§Œì¡±ë„", "ì¼/í•™ì—… ë§Œì¡±ë„", "ì—¬ê°€í™œë™ ë§Œì¡±ë„",

        # ê°€ì •Â·ë°°ê²½ ìš”ì¸(ê¸°ì¡´ + í™•ì¥)
        "ê°€êµ¬ì›", "ê°€êµ¬ì› ìˆ˜", "ê°€êµ¬", "ê°€êµ¬ì£¼",
        "ì†Œë“", "ê°€êµ¬ì†Œë“", "ê°€êµ¬ ì›”ì†Œë“",
        "ë§ë²Œì´", "í•œë¶€ëª¨", "ì–‘ìœ¡ì", "ì£¼ ì–‘ìœ¡ì",
    ],

    "ì¡°ì‚¬": [
        # ì¡°ì‚¬ ìš´ì˜/ì„¤ê³„(ë³´ê³ ì„œ í‘œí˜„ ê·¸ëŒ€ë¡œ + ë³€í˜•)
        "ì¡°ì‚¬ê°œìš”", "ì¡°ì‚¬ ê°œìš”",
        "ì¡°ì‚¬ë°©ë²•", "ìë£Œìˆ˜ì§‘", "ìë£Œ ìˆ˜ì§‘", "ìë£Œì²˜ë¦¬", "ìë£Œ ì²˜ë¦¬",
        "ê°€êµ¬ë°©ë¬¸ ë©´ì ‘ì¡°ì‚¬", "ê°€êµ¬ ë°©ë¬¸", "ë©´ì ‘ì¡°ì‚¬", "ë©´ì ‘ ì¡°ì‚¬",
        "êµ¬ì¡°í™”ëœ ì„¤ë¬¸ì§€", "ì¡°ì‚¬í‘œ", "ê°€êµ¬ì£¼ìš© ì„¤ë¬¸ì§€", "ê°€êµ¬ì› ì„¤ë¬¸ì§€",
        "ì¡°ì‚¬ê¸°ê°„", "ì¡°ì‚¬ ê¸°ì¤€ì‹œì ", "2024ë…„ 9ì›”~11ì›”",

        # í‘œë³¸ì„¤ê³„/ì¶”ì •
        "í‘œë³¸", "í‘œë³¸ì„¤ê³„", "í‘œë³¸ ì„¤ê³„", "í‘œë³¸ë°°ë¶„", "í‘œë³¸ ë°°ë¶„", "í‘œë³¸ì¶”ì¶œ", "í‘œë³¸ ì¶”ì¶œ",
        "ì¡°ì‚¬êµ¬", "ê°€êµ¬ëª…ë¶€", "ê°€êµ¬ ëª…ë¶€", "ì¸êµ¬ì£¼íƒì´ì¡°ì‚¬", 
        "ì¸µí™”", "ì¸µë³„", "ì£¼íƒìœ í˜•", "ì•„íŒŒíŠ¸", "ë³´í†µì¡°ì‚¬êµ¬",
        "ê°€ì¤‘ì¹˜", "ê°€ì¤‘ì¹˜ ì‚°ì •", "ëª¨ìˆ˜ì¶”ì •", "ëª¨ìˆ˜ ì¶”ì •", "ì¶”ì •ì‹",
        "ì‹ ë¢°ìˆ˜ì¤€", "í‘œë³¸ì˜¤ì°¨", "í‘œì§‘ì˜¤ì°¨", "ë°˜ì˜¬ë¦¼", "ë³µìˆ˜ì‘ë‹µ",
    ],

    # ---- ì¶”ê°€ í† í”½(í•„ìš” ì‹œ) ----
    "ì˜ˆë°©Â·ìƒë‹´": [
        "ì˜ˆë°©êµìœ¡", "ì˜ˆë°© êµìœ¡", "ìƒë‹´", "í”„ë¡œê·¸ë¨",
        "ì¸ì§€ìœ¨", "ì´ìš©ê²½í—˜", "ê²½í—˜ë¥ ", "ë„ì›€ì •ë„", "ì°¸ì—¬ ì˜í–¥",
        "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì˜ˆë°© ê¸°ê´€", "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì˜ˆë°© í”„ë¡œê·¸ë¨",
        "ìŠ¤ë§ˆíŠ¸ì‰¼ì„¼í„°",
    ],

    "í•´ê²°ë°©ì•ˆ": [
        "ê³¼ì˜ì¡´ ì‹¬ê°ì„± ì¸ì‹",
        "ê³¼ì˜ì¡´ í•´ì†Œ ë°©ì•ˆ", "ëŒ€ì²˜ë°©ì•ˆ", "ëŒ€ì²˜ ë°©ì•ˆ",
        "ë¬¸ì œí•´ê²° ì£¼ì²´", "ë¬¸ì œ í•´ê²° ì£¼ì²´",
        "ê°œì¸ì˜ í•´ì†Œë°©ì•ˆ", "ê°œì¸ì˜ ì¥ì• ìš”ì¸",
        "ê¸°ì—…ì˜ í•´ì†Œë°©ì•ˆ", "ì •ë¶€ì˜ í•´ì†Œë°©ì•ˆ", "êµìœ¡ì‹œì„¤ì˜ í•´ì†Œë°©ì•ˆ",
        "ë””ì§€í„¸ ë””í†¡ìŠ¤", "ë””ì§€í„¸ ë””í†¡ìŠ¤ ê²½í—˜",
    ],
}

# =========================================================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# =========================================================
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "clarification_context" not in st.session_state:
    st.session_state.clarification_context = None

# =========================================================
# LangGraph State ì •ì˜
# =========================================================
ValidationResult = Literal["PASS", "FAIL_NO_EVIDENCE", "FAIL_UNCLEAR", "FAIL_FORMAT"]

class GraphState(TypedDict):
    input: str
    chat_history: List[BaseMessage]
    session_id: str
    intent_raw: Optional[str]
    intent: Optional[str]
    is_chat_reference: Optional[bool]
    followup_type: Optional[str]
    plan: Optional[Dict[str, Any]]
    resolved_question: Optional[str]
    previous_context: Optional[str]
    rewritten_queries: Optional[List[str]]
    retrieval: Optional[Dict[str, Any]]
    context: Optional[str]
    reranked_docs: Optional[List[Document]]
    compressed_context: Optional[str]
    sanitized_context: Optional[str]
    draft_answer: Optional[str]
    safety_passed: Optional[bool]
    safety_issues: Optional[List[str]]
    validation_result: Optional[ValidationResult]
    validation_reason: Optional[str]
    validator_output: Optional[Dict[str, Any]]
    final_answer: Optional[str]
    retry_count: Optional[int]
    retry_type: Optional[str]
    pending_clarification: Optional[str]
    clarification_context: Optional[Dict[str, Any]]
    used_default_years: Optional[bool]  # v5.1: ê¸°ë³¸ ì—°ë„ ì‚¬ìš© í”Œë˜ê·¸
    debug_info: Optional[Dict[str, Any]]

# =========================================================
# Hugging Faceì—ì„œ DB ë‹¤ìš´ë¡œë“œ
# =========================================================
@st.cache_resource
def download_chroma_db():
    if os.path.exists(LOCAL_DB_PATH) and os.listdir(LOCAL_DB_PATH):
        return LOCAL_DB_PATH, None
    
    try:
        from huggingface_hub import snapshot_download
        downloaded_path = snapshot_download(
            repo_id=HF_REPO_ID,
            repo_type="dataset",
            local_dir=LOCAL_DB_PATH,
            local_dir_use_symlinks=False
        )
        return downloaded_path, None
    except Exception as e:
        return None, str(e)

# =========================================================
# ì´ˆê¸°í™” í•¨ìˆ˜
# =========================================================
@st.cache_resource
def init_resources():
    api_key = None
    try:
        api_key = st.secrets.get("OPENAI_API_KEY")
    except:
        pass
    
    os.environ['OPENAI_API_KEY'] = api_key
    
    if not os.path.exists(LOCAL_DB_PATH):
        return None, None, f"Chroma DBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {LOCAL_DB_PATH}"
    
    try:
        embedding = OpenAIEmbeddings(model='text-embedding-3-large')
        vectorstore = Chroma(
            persist_directory=LOCAL_DB_PATH,
            embedding_function=embedding,
            collection_name="pdf_pages_with_summary_v2"
        )
        
        llms = {
            "router": ChatOpenAI(model="gpt-4o-mini", temperature=0, max_tokens=50),
            "casual": ChatOpenAI(model="gpt-4o-mini", temperature=0.5, max_tokens=500),
            "main": ChatOpenAI(model="gpt-4o-mini", temperature=0.2, max_tokens=4000),
            "planner": ChatOpenAI(model="gpt-4o-mini", temperature=0, max_tokens=1000),
            "rewrite": ChatOpenAI(model="gpt-4o-mini", temperature=0, max_tokens=500),
        }
        
        return vectorstore, llms, None
    except Exception as e:
        return None, None, str(e)

# =========================================================
# í—¬í¼ í•¨ìˆ˜ë“¤
# =========================================================
def is_chat_reference_question(user_input: str) -> bool:
    name_intro_patterns = [
        r"(ë‚´|ì œ)\s*ì´ë¦„ì€?\s*[ê°€-í£a-zA-Z]+",
        r"(ì €ëŠ”|ë‚˜ëŠ”)\s*[ê°€-í£a-zA-Z]+",
    ]
    for p in name_intro_patterns:
        if re.search(p, user_input):
            return False
    
    patterns = [
        r"(ë‚´|ì œ)\s*ì´ë¦„\s*(ë­|ë­”|ì•Œ|ê¸°ì–µ)",
        r"(ë‚´|ì œ)\s*ì´ë¦„\s*[?]",
        r"ë­ë¼ê³ \s*(í–ˆ|ë¬¼ì–´|ë§)",
        r"ì•„ê¹Œ", r"ë°©ê¸ˆ", r"ì´ì „ì—",
    ]
    for p in patterns:
        if re.search(p, user_input):
            return True
    return False

def parse_year_range(text: str) -> List[int]:
    years = set()
    range_patterns = [
        r"(20[2][0-4])\s*ë…„?\s*(?:ì—ì„œ|ë¶€í„°|~|-|â€“)\s*(20[2][0-4])\s*ë…„?\s*(?:ê¹Œì§€)?",
        r"(20[2][0-4])\s*(?:~|-|â€“)\s*(20[2][0-4])",
    ]
    for pattern in range_patterns:
        matches = re.findall(pattern, text)
        for m in matches:
            start, end = int(m[0]), int(m[1])
            for y in range(start, end + 1):
                if y in YEAR_TO_FILENAME:
                    years.add(y)
    
    single_years = re.findall(r"\b(20[2][0-4])\s*ë…„?\b", text)
    for y in single_years:
        yi = int(y)
        if yi in YEAR_TO_FILENAME:
            years.add(yi)
    
    return sorted(list(years))

def classify_followup_type(user_input: str, prev_context: Dict[str, Any]) -> str:
    if not prev_context.get("last_topic"):
        return "none"
    
    has_new_topic_keyword = False
    for keywords in TOPIC_KEYWORDS.values():
        for kw in keywords:
            if kw in user_input and kw not in str(prev_context.get("last_topic_core", "")):
                has_new_topic_keyword = True
                break
    
    if len(user_input) >= 30 and has_new_topic_keyword:
        return "none"
    
    target_patterns = [
        r"^(ì²­ì†Œë…„|ìœ ì•„ë™|ì„±ì¸|60ëŒ€|ëŒ€í•™ìƒ|ì¤‘í•™ìƒ|ê³ ë“±í•™ìƒ|ì´ˆë“±í•™ìƒ|ë‚¨ì„±|ì—¬ì„±)[ì€ì˜]?\s*[?]?$",
        r"^(ì²­ì†Œë…„|ìœ ì•„ë™|ì„±ì¸|60ëŒ€)[ì€ì˜]?\s*(ì–´ë•Œ|ì–´ë–»ê²Œ|ì–´ë–¤ê°€|ê²°ê³¼|ê¸°ì¤€|ê²½ìš°)",
    ]
    for p in target_patterns:
        if re.search(p, user_input):
            return "target_change"
    
    if len(user_input) <= 20:
        for keywords in TARGET_KEYWORDS.values():
            for kw in keywords:
                if kw in user_input:
                    return "target_change"
    
    year_patterns = [
        r"^(20[2][0-4])ë…„?\s*[ì€ì˜]?\s*[?]?$",
        r"^(20[2][0-4])ë…„?\s*(ì–´ë•Œ|ì–´ë–»ê²Œ|ê²°ê³¼|ê¸°ì¤€)",
    ]
    for p in year_patterns:
        if re.search(p, user_input):
            return "year_change"
    
    if len(user_input) <= 15:
        years = parse_year_range(user_input)
        if years:
            return "year_change"
    
    detail_patterns = [
        r"(ë”|ì¢€)\s*(ìì„¸íˆ|êµ¬ì²´ì |ìƒì„¸)",
        r"(ì™œ|ì›ì¸|ì´ìœ ).*[?]",
    ]
    for p in detail_patterns:
        if re.search(p, user_input):
            return "detail_request"
    
    if len(user_input) <= 15 and re.search(r"[?]$", user_input):
        return "detail_request"
    
    return "none"

def extract_previous_context(chat_history: List[BaseMessage]) -> Dict[str, Any]:
    context = {
        "user_name": None,
        "last_topic": None,
        "last_topic_core": None,
        "last_target": None,
        "last_years": [],
    }
    
    if not chat_history:
        return context
    
    for msg in chat_history:
        if isinstance(msg, HumanMessage):
            name_match = re.search(r"(?:ë‚´\s*ì´ë¦„ì€?|ì €ëŠ”?|ë‚˜ëŠ”?)\s*([ê°€-í£a-zA-Z]+)", msg.content)
            if name_match:
                context["user_name"] = name_match.group(1)
    
    human_msgs = [m for m in chat_history if isinstance(m, HumanMessage)][-2:]
    
    for msg in reversed(human_msgs):
        content = msg.content
        
        if not context["last_topic"]:
            context["last_topic"] = content[:300]
        
        years = parse_year_range(content)
        if years and not context["last_years"]:
            context["last_years"] = years
        
        if not context["last_target"]:
            for keywords in TARGET_KEYWORDS.values():
                for kw in keywords:
                    if kw in content:
                        context["last_target"] = kw
                        break
                if context["last_target"]:
                    break
        
        if not context["last_topic_core"]:
            topic_parts = []
            for keywords in TOPIC_KEYWORDS.values():
                for kw in keywords:
                    if kw in content:
                        topic_parts.append(kw)
            if topic_parts:
                context["last_topic_core"] = " ".join(topic_parts[:3])
    
    return context

def _keyword_boost_score(doc: Document, query: str) -> float:
    text = (doc.page_content or "").lower()
    query_terms = re.findall(r'[ê°€-í£a-zA-Z0-9]+', query.lower())
    boost = 0.0
    for term in query_terms:
        if len(term) >= 2 and term in text:
            boost += 0.02
    return min(boost, 0.15)

# =========================================================
# í…Œì´ë¸” íŒŒì‹± ë° ë Œë”ë§
# =========================================================
def parse_markdown_table(text: str) -> List[Dict[str, Any]]:
    tables = []
    lines = text.split('\n')
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        if line.startswith('|') and line.endswith('|'):
            table_lines = []
            start_idx = i
            while i < len(lines):
                line = lines[i].strip()
                if line.startswith('|') and line.endswith('|'):
                    table_lines.append(line)
                    i += 1
                elif line.startswith('|---') or line.startswith('| ---'):
                    i += 1
                    continue
                else:
                    break
            
            if len(table_lines) >= 2:
                header_line = table_lines[0]
                headers = [h.strip() for h in header_line.split('|')[1:-1]]
                data_rows = []
                for row_line in table_lines[1:]:
                    if '---' in row_line:
                        continue
                    cells = [c.strip() for c in row_line.split('|')[1:-1]]
                    if len(cells) == len(headers):
                        data_rows.append(cells)
                
                if headers and data_rows:
                    tables.append({
                        'headers': headers,
                        'rows': data_rows,
                        'start_idx': start_idx,
                        'end_idx': i
                    })
        else:
            i += 1
    return tables

def render_answer_with_tables(answer: str) -> None:
    tables = parse_markdown_table(answer)
    if not tables:
        st.markdown(answer)
        return
    
    lines = answer.split('\n')
    current_pos = 0
    
    for table in tables:
        before_text = '\n'.join(lines[current_pos:table['start_idx']])
        if before_text.strip():
            st.markdown(before_text)
        
        try:
            df = pd.DataFrame(table['rows'], columns=table['headers'])
            st.dataframe(df, use_container_width=True, hide_index=True)
        except:
            st.markdown("| " + " | ".join(table['headers']) + " |")
            for row in table['rows']:
                st.markdown("| " + " | ".join(row) + " |")
        
        current_pos = table['end_idx']
    
    after_text = '\n'.join(lines[current_pos:])
    if after_text.strip():
        st.markdown(after_text)

# =========================================================
# í”„ë¡¬í”„íŠ¸ ì •ì˜
# =========================================================
def get_router_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” ë¼ìš°í„°ì…ë‹ˆë‹¤.\n"
         "ë¶„ë¥˜: SMALLTALK / RAG / CHAT_REF / OFFTOPIC\n"
         "ì¶œë ¥: ë¶„ë¥˜ëª…ë§Œ"
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])

def get_smalltalk_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         f"ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n{BOT_IDENTITY}\n"
         "ì¸ì‚¬ì—ëŠ” ê°„ê²°í•˜ê²Œ ì‘ëŒ€í•˜ê³  ì˜ˆì‹œ ì§ˆë¬¸ì„ ì œì•ˆí•˜ì„¸ìš”."
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])

def get_offtopic_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n"
         "í•´ë‹¹ ì§ˆë¬¸ì€ ì „ë¬¸ ë¶„ì•¼ê°€ ì•„ë‹™ë‹ˆë‹¤. ì •ì¤‘í•˜ê²Œ ì•ˆë‚´í•˜ì„¸ìš”."
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])

def get_planner_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½ê¸°ì…ë‹ˆë‹¤. JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n"
         "í—ˆìš© íŒŒì¼ëª…:\n" +
         "\n".join([f"- {y}ë…„: {fn}" for y, fn in YEAR_TO_FILENAME.items()]) +
         "\n\nJSON: {{\"resolved_question\": \"...\", \"years\": [...], "
         "\"file_name_filters\": [...], \"queries\": [...]}}"
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human",
         "ì§ˆë¬¸: {input}\ní›„ì†ì§ˆë¬¸ ìœ í˜•: {followup_type}\n"
         "ì´ì „ ì£¼ì œ: {topic_core}\nì´ì „ ëŒ€ìƒ: {last_target}\nì´ì „ ì—°ë„: {last_years}\n\nJSON:")
    ])

def get_rewrite_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
         "ë¶ˆí•„ìš”í•œ ì¡°ì‚¬/ì–´ë¯¸ ì œê±°, í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ, ë™ì˜ì–´ í™•ì¥.\n"
         "JSON: {{\"optimized_queries\": [\"ì¿¼ë¦¬1\", \"ì¿¼ë¦¬2\", ...]}}"
        ),
        ("human",
         "ì›ë³¸ ì§ˆë¬¸: {resolved_question}\nì›ë³¸ ì¿¼ë¦¬: {queries}\nì—°ë„: {years}\n\nJSON:")
    ])

def get_answer_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n\n"
         "ì›ì¹™:\n"
         "1. CONTEXTì—ì„œ ìˆ˜ì¹˜ ì¸ìš© í•„ìˆ˜\n"
         "2. ì¶œì²˜(íŒŒì¼ëª… p.í˜ì´ì§€) í•„ìˆ˜\n"
         "3. ë³€í™”ëŸ‰(%p) ëª…ì‹œ\n"
         "4. CONTEXTì— ì—†ìœ¼ë©´ 'ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤' ëª…ì‹œ"
        ),
        ("human",
         "[ì§ˆë¬¸]\n{input}\n\n[CONTEXT]\n{context}\n\në‹µë³€:")
    ])

def get_answer_retry_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n\n"
         "âš ï¸ ì´ì „ ë¬¸ì œ: {previous_issue}\n\n"
         "ìˆ˜ì • ì§€ì¹¨:\n"
         "1. ëª¨ë“  ìˆ˜ì¹˜ì— ì¶œì²˜ í˜•ì‹: (íŒŒì¼ëª….pdf p.00)\n"
         "2. CONTEXTì—ì„œ ì§ì ‘ ì¸ìš©ë§Œ\n"
         "3. ì—†ëŠ” ì •ë³´ëŠ” 'í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤' ëª…ì‹œ"
        ),
        ("human",
         "[ì§ˆë¬¸]\n{input}\n\n[CONTEXT]\n{context}\n\nìˆ˜ì •ëœ ë‹µë³€:")
    ])

def get_validator_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "ë‹µë³€ í’ˆì§ˆ ê²€ìˆ˜ê¸°ì…ë‹ˆë‹¤.\n\n"
         "ë¶„ë¥˜:\n"
         "- PASS: ì–‘í˜¸\n"
         "- FAIL_NO_EVIDENCE: ê·¼ê±° ë¶€ì¡± (ê²€ìƒ‰ ì¬ì‹œë„ í•„ìš”)\n"
         "- FAIL_UNCLEAR: ì§ˆë¬¸ ë¶ˆëª…í™• (ëª…í™•í™” í•„ìš”)\n"
         "- FAIL_FORMAT: í˜•ì‹ ë¬¸ì œ (ì¬ì‘ì„± í•„ìš”)\n\n"
         "JSON: {{\"result\": \"PASS|FAIL_...\", \"reason\": \"...\", "
         "\"clarify_question\": \"...\", \"corrected_answer\": \"...\"}}"
        ),
        ("human",
         "[ì§ˆë¬¸]\n{input}\n\n[CONTEXT]\n{context}\n\n[ë‹µë³€]\n{answer}\n\nJSON:")
    ])

# =========================================================
# ë…¸ë“œ í•¨ìˆ˜ë“¤ ìƒì„±
# =========================================================
def create_node_functions(vectorstore, llms, status_placeholder):
    
    def update_status(message: str, retry_info: str = ""):
        retry_badge = f'<span class="retry-badge">{retry_info}</span>' if retry_info else ""
        status_placeholder.markdown(f"""
        <div class="status-box">ğŸ”„ {message} {retry_badge}</div>
        """, unsafe_allow_html=True)
    
    # ----- ë…¸ë“œ 1: ë¼ìš°í„° -----
    def route_intent(state: GraphState) -> GraphState:
        update_status("ì§ˆë¬¸ ë¶„ì„ ì¤‘...")
        try:
            user_input = state["input"]
            chat_history = state.get("chat_history", [])
            
            state["retry_count"] = state.get("retry_count") or 0
            
            # Clarification ì‘ë‹µ ì²˜ë¦¬
            if state.get("clarification_context"):
                state["intent"] = "RAG"
                state["clarification_context"] = None
                return state
            
            if is_chat_reference_question(user_input):
                state["intent"] = "CHAT_REF"
                state["followup_type"] = "none"
                return state
            
            prev_ctx = extract_previous_context(chat_history)
            followup_type = classify_followup_type(user_input, prev_ctx)
            state["followup_type"] = followup_type
            
            rag_keywords = [
                # í•µì‹¬ ì£¼ì œ/ìš©ì–´
                "ìŠ¤ë§ˆíŠ¸í°", "ê³¼ì˜ì¡´", "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´", "ê³¼ë‹¤ì´ìš©", "ê³¼ì˜ì¡´ìœ„í—˜êµ°", "ê³ ìœ„í—˜êµ°", "ì ì¬ì ìœ„í—˜êµ°", "ì¼ë°˜ì‚¬ìš©ìêµ°",
                "ì¡°ì ˆì‹¤íŒ¨", "í˜„ì €ì„±", "ë¬¸ì œì  ê²°ê³¼",
                "ì¡°ì‚¬", "ì‹¤íƒœì¡°ì‚¬", "ì¡°ì‚¬ê°œìš”", "ì¡°ì‚¬ë°©ë²•", "ì¡°ì‚¬ëŒ€ìƒ", "ëª¨ì§‘ë‹¨", "í‘œë³¸", "í‘œë³¸ì„¤ê³„", "í‘œë³¸ì¶”ì¶œ", "ê°€ì¤‘ì¹˜", "ëª¨ìˆ˜ì¶”ì •",
                
                # ì§€í‘œ/í‘œí˜„
                "ê³¼ì˜ì¡´ë¥ ", "ë¹„ìœ¨", "ë¥ ", "%", "%p", "ë‹¨ìœ„", "ì ìˆ˜", "ì´ì ", "í‰ê· ", "4ì ë§Œì ", "ê¸°ì¤€ì ìˆ˜", "ì—­ë¬¸í•­",
                
                # ëŒ€ìƒ/ë¶„ë¥˜
                "ìœ ì•„ë™", "ì˜ìœ ì•„", "ì•„ë™", "ì²­ì†Œë…„", "ì„±ì¸", "60ëŒ€", "ê³ ë ¹ì¸µ",
                "ì´ˆë“±í•™ìƒ", "ì¤‘í•™ìƒ", "ê³ ë“±í•™ìƒ", "ëŒ€í•™ìƒ",
                "ì„±ë³„", "ë‚¨ì„±", "ì—¬ì„±", "ì§€ì—­", "ë„ì‹œê·œëª¨", "ëŒ€ë„ì‹œ", "ì¤‘ì†Œë„ì‹œ", "ìë©´ì§€ì—­",
                
                # ì´ìš©/ì¡°ì ˆ/ì‹¬ì¸µë¬¸í•­
                "ì´ìš©ì‹œê°„", "ì´ìš©ì‹œê°„ ì¡°ì ˆ", "ì¡°ì ˆ ì–´ë ¤ì›€", "ë³¸ì¸ ì˜ì§€ëŒ€ë¡œ ì¡°ì ˆ",
                "ì‚¬ìš©ì¡°ì ˆ", "ìƒí™œ ìš°ì„ ì„±", "íí•´ ê²½í—˜", "ì‹ ì²´ ê±´ê°•", "ì •ì‹  ê±´ê°•", "ëŒ€ì¸ê´€ê³„", "ìƒì‚°ì„± ì €í•˜",
                
                # ì½˜í…ì¸ /í”Œë«í¼
                "ì½˜í…ì¸ ", "ì´ìš©ë¥ ", "ì´ìš©ì •ë„", "ìƒí™œì— ë„ì›€ì´ ë˜ëŠ” ì½˜í…ì¸ ", "ë¶€ì‘ìš© ìš°ë ¤ ì½˜í…ì¸ ", "ìµœê·¼ 1ë…„ê°„ ì´ìš©ëŸ‰ ì¦ê°€",
                "ì˜¨ë¼ì¸ ë™ì˜ìƒ ì„œë¹„ìŠ¤", "OVS", "ìˆí¼", "ì•Œê³ ë¦¬ì¦˜", "ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜",
                "ìœ íŠœë¸Œ ì‡¼ì¸ ", "ì¸ìŠ¤íƒ€ê·¸ë¨ ë¦´ìŠ¤", "í‹±í†¡", "ë„¤ì´ë²„ í´ë¦½", "í˜ì´ìŠ¤ë¶ ë¦´ìŠ¤",
                "SNS", "ë©”ì‹ ì €", "ê²Œì„", "ì˜í™”/TV/ë™ì˜ìƒ", "ë‰´ìŠ¤ë³´ê¸°", "ì‡¼í•‘", "íˆ¬ì ë° ìì‚°ê´€ë¦¬", "ì„±ì¸ìš© ì½˜í…ì¸ ", "ì‚¬í–‰ì„± ê²Œì„",
                "ìƒì„±í˜• AIì„œë¹„ìŠ¤",

                # ì˜ˆë°©/ìƒë‹´/í•´ê²°
                "ì˜ˆë°©êµìœ¡", "ìƒë‹´", "í”„ë¡œê·¸ë¨", "ì¸ì§€ìœ¨", "ê²½í—˜ë¥ ", "ë„ì›€ì •ë„", "ì°¸ì—¬ ì˜í–¥",
                "ëŒ€ì²˜ë°©ì•ˆ", "í•´ì†Œ ë°©ì•ˆ", "ë¬¸ì œí•´ê²° ì£¼ì²´", "ê¸°ì—…", "ì •ë¶€", "êµìœ¡ì‹œì„¤", "ë””ì§€í„¸ ë””í†¡ìŠ¤",
                # ë¬¸ì„œ ë‚´ ì°¸ì¡° í† í°(ì§ˆì˜ì— â€œí‘œ/ê·¸ë¦¼ ë²ˆí˜¸â€ë¡œ ë“¤ì–´ì˜¤ëŠ” ê²½ìš° ëŒ€ë¹„)
                "í‘œ", "ê·¸ë¦¼", "ë¬¸í•­", "ìš”ì¸ë³„ ì†ì„±", "ë¬¸í•­ë³„ ì†ì„±",
            ]

            
            if re.search(r"\b(20[2][0-4])\s*ë…„?\b", user_input):
                state["intent"] = "RAG"
                return state
            
            if any(kw in user_input for kw in rag_keywords):
                state["intent"] = "RAG"
                return state
            
            if followup_type != "none":
                state["intent"] = "RAG"
                return state
            
            result = (get_router_prompt() | llms["router"] | StrOutputParser()).invoke({
                "input": user_input,
                "chat_history": chat_history
            })
            state["intent_raw"] = result.strip().upper()
            
            if state["intent_raw"] in ("SMALLTALK", "RAG", "OFFTOPIC", "CHAT_REF"):
                state["intent"] = state["intent_raw"]
            else:
                state["intent"] = "RAG"
            
            return state
        except Exception as e:
            state["intent"] = "RAG"
            state["followup_type"] = "none"
            return state
    
    # ----- ë…¸ë“œ 2a: SMALLTALK -----
    def handle_smalltalk(state: GraphState) -> GraphState:
        update_status("ì‘ë‹µ ìƒì„± ì¤‘...")
        try:
            answer = (get_smalltalk_prompt() | llms["casual"] | StrOutputParser()).invoke({
                "input": state["input"],
                "chat_history": state.get("chat_history", [])
            })
            state["final_answer"] = answer
            return state
        except Exception as e:
            state["final_answer"] = f"ì˜¤ë¥˜: {e}"
            return state
    
    # ----- ë…¸ë“œ 2b: OFFTOPIC -----
    def handle_offtopic(state: GraphState) -> GraphState:
        update_status("ì‘ë‹µ ìƒì„± ì¤‘...")
        try:
            answer = (get_offtopic_prompt() | llms["casual"] | StrOutputParser()).invoke({
                "input": state["input"],
                "chat_history": state.get("chat_history", [])
            })
            state["final_answer"] = answer
            return state
        except Exception as e:
            state["final_answer"] = f"ì˜¤ë¥˜: {e}"
            return state
    
    # ----- ë…¸ë“œ 2c: CHAT_REF -----
    def handle_chat_reference(state: GraphState) -> GraphState:
        update_status("ëŒ€í™” ê¸°ë¡ í™•ì¸ ì¤‘...")
        try:
            chat_history = state.get("chat_history", [])
            user_input = state["input"]
            prev_ctx = extract_previous_context(chat_history)
            
            if re.search(r"(ë‚´|ì œ)\s*ì´ë¦„", user_input):
                if prev_ctx["user_name"]:
                    state["final_answer"] = f"{prev_ctx['user_name']}ë‹˜ìœ¼ë¡œ ë§ì”€í•˜ì…¨ìŠµë‹ˆë‹¤."
                else:
                    state["final_answer"] = "ì•„ì§ ì´ë¦„ì„ ë§ì”€í•´ì£¼ì‹œì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                return state
            
            state["final_answer"] = "ì´ì „ ëŒ€í™” ì°¸ì¡°ê°€ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            return state
        except Exception as e:
            state["final_answer"] = f"ì˜¤ë¥˜: {e}"
            return state
    
    # ----- ë…¸ë“œ 3: í”Œë˜ë„ˆ -----
    def plan_search(state: GraphState) -> GraphState:
        update_status("ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½ ì¤‘...")
        try:
            user_input = state["input"]
            chat_history = state.get("chat_history", [])
            followup_type = state.get("followup_type", "none")
            
            prev_ctx = extract_previous_context(chat_history)
            
            if followup_type == "none":
                topic_core, last_target, last_years = "", "", []
            else:
                topic_core = prev_ctx.get("last_topic_core", "") or ""
                last_target = prev_ctx.get("last_target", "") or ""
                last_years = prev_ctx.get("last_years", [])
            
            result = (get_planner_prompt() | llms["planner"] | StrOutputParser()).invoke({
                "input": user_input,
                "chat_history": chat_history[-4:],
                "followup_type": followup_type,
                "topic_core": topic_core,
                "last_target": last_target,
                "last_years": str(last_years),
            })
            
            json_match = re.search(r'\{[\s\S]*\}', result)
            if json_match:
                result = json_match.group()
            
            plan = json.loads(result)
            
            years = plan.get('years', [])
            input_years = parse_year_range(user_input)
            years = sorted(list(set([y for y in (years + input_years) if y in YEAR_TO_FILENAME])))
            
            # âœ… v5.1: ì—°ë„ ë¯¸ì§€ì • ì‹œ ê¸°ë³¸ê°’ (ìµœê·¼ 2ë…„) ì„¤ì •
            used_default_years = False
            if not years:
                years = [2023, 2024]
                used_default_years = True
            
            state["used_default_years"] = used_default_years
            
            fns = [fn for fn in plan.get("file_name_filters", []) if fn in ALLOWED_FILES]
            if years and not fns:
                fns = [YEAR_TO_FILENAME[y] for y in years]
            
            queries = [str(q).strip() for q in plan.get('queries', []) if str(q).strip()]
            resolved_q = plan.get("resolved_question", user_input) or user_input
            
            while len(queries) < 3:
                queries.append(resolved_q)
            
            state["plan"] = {
                "years": years,
                "file_name_filters": fns,
                "queries": queries[:3],
                "resolved_question": resolved_q,
                "used_default_years": used_default_years,
            }
            state["resolved_question"] = resolved_q
            return state
            
        except Exception as e:
            years = parse_year_range(state["input"])
            
            # âœ… í´ë°±ì—ì„œë„ ê¸°ë³¸ ì—°ë„ ì ìš©
            used_default_years = False
            if not years:
                years = [2023, 2024]
                used_default_years = True
            
            fns = [YEAR_TO_FILENAME[y] for y in years if y in YEAR_TO_FILENAME]
            state["plan"] = {
                "years": years,
                "file_name_filters": fns,
                "queries": [state["input"]] * 3,
                "resolved_question": state["input"],
                "used_default_years": used_default_years,
            }
            state["resolved_question"] = state["input"]
            state["used_default_years"] = used_default_years
            return state
    
    # ----- ë…¸ë“œ 4: Query Rewrite -----
    def query_rewrite(state: GraphState) -> GraphState:
        update_status("ì¿¼ë¦¬ ìµœì í™” ì¤‘...")
        try:
            plan = state["plan"]
            queries = plan.get("queries", [])
            resolved_q = plan.get("resolved_question", "")
            years = plan.get("years", [])
            
            # ë©€í‹°ì—°ë„ ì¿¼ë¦¬ ì¶”ê°€
            if len(years) > 1:
                base_query_clean = re.sub(r'20[2][0-4]ë…„?', '', resolved_q).strip()
                for y in years:
                    year_query = f"{y}ë…„ {base_query_clean}"
                    if year_query not in queries:
                        queries.append(year_query)
            
            result = (get_rewrite_prompt() | llms["rewrite"] | StrOutputParser()).invoke({
                "resolved_question": resolved_q,
                "queries": str(queries),
                "years": str(years),
            })
            
            json_match = re.search(r'\{[\s\S]*\}', result)
            if json_match:
                result = json_match.group()
            
            optimized = json.loads(result)
            rewritten = optimized.get("optimized_queries", queries)
            
            if not isinstance(rewritten, list) or not rewritten:
                rewritten = queries
            
            # ì¤‘ë³µ ì œê±°
            unique_queries = list(dict.fromkeys(rewritten))
            
            state["rewritten_queries"] = unique_queries[:6]
            state["plan"]["queries"] = unique_queries[:6]
            return state
            
        except Exception as e:
            state["rewritten_queries"] = state["plan"].get("queries", [])
            return state
    
    # ----- ë…¸ë“œ 5: ê²€ìƒ‰ -----
    def retrieve_documents(state: GraphState) -> GraphState:
        retry_count = state.get("retry_count", 0)
        retry_info = f"ì¬ì‹œë„ #{retry_count}" if retry_count > 0 else ""
        update_status("ë³´ê³ ì„œ ê²€ìƒ‰ ì¤‘...", retry_info)
        
        try:
            plan = state["plan"]
            target_files = plan.get("file_name_filters", [])
            queries = state.get("rewritten_queries") or plan.get("queries", [])
            resolved_q = plan.get("resolved_question", "")
            
            # ì¬ì‹œë„ ì‹œ íŒŒë¼ë¯¸í„° ì¦ê°€
            if retry_count > 0 and state.get("retry_type") == "retrieve":
                k_per_query = RETRY_K_PER_QUERY
                top_parents = RETRY_TOP_PARENTS
                top_parents_per_file = RETRY_TOP_PARENTS_PER_FILE
            else:
                k_per_query = DEFAULT_K_PER_QUERY
                top_parents = DEFAULT_TOP_PARENTS
                top_parents_per_file = DEFAULT_TOP_PARENTS_PER_FILE
            
            all_docs = []
            files_searched = []
            
            if target_files:
                for fn in target_files:
                    file_filter = {'$and': [
                        {'doc_type': {"$in": SUMMARY_TYPES}},
                        {'file_name': fn}
                    ]}
                    
                    file_docs = []
                    seen_keys = set()
                    
                    for q in queries:
                        if not q:
                            continue
                        try:
                            hits = vectorstore.similarity_search_with_relevance_scores(
                                q, k=k_per_query, filter=file_filter
                            )
                            for doc, score in hits:
                                key = f"{doc.metadata.get('parent_id')}|{doc.metadata.get('page')}"
                                if key not in seen_keys:
                                    doc.metadata["_score"] = float(score)
                                    doc.metadata["_source_file"] = fn
                                    file_docs.append(doc)
                                    seen_keys.add(key)
                        except:
                            pass
                    
                    for doc in file_docs:
                        boost = _keyword_boost_score(doc, resolved_q)
                        doc.metadata["_final_score"] = doc.metadata.get("_score", 0) + boost
                    
                    file_docs.sort(key=lambda d: d.metadata.get("_final_score", 0), reverse=True)
                    all_docs.extend(file_docs[:top_parents_per_file * 2])
                    
                    if file_docs:
                        files_searched.append(fn)
            else:
                base_filter = {'doc_type': {"$in": SUMMARY_TYPES}}
                seen_keys = set()
                
                for q in queries:
                    if not q:
                        continue
                    hits = vectorstore.similarity_search_with_relevance_scores(
                        q, k=k_per_query, filter=base_filter
                    )
                    for doc, score in hits:
                        key = f"{doc.metadata.get('parent_id')}|{doc.metadata.get('page')}"
                        if key not in seen_keys:
                            doc.metadata["_score"] = float(score)
                            all_docs.append(doc)
                            seen_keys.add(key)
                
                for doc in all_docs:
                    boost = _keyword_boost_score(doc, resolved_q)
                    doc.metadata["_final_score"] = doc.metadata.get("_score", 0) + boost
                
                files_searched = ["ì „ì²´"]
            
            all_docs.sort(key=lambda d: d.metadata.get("_final_score", 0), reverse=True)
            
            # Parent ID ì„ ì •
            parent_ids = []
            seen_pid = set()
            
            if target_files:
                for fn in target_files:
                    for doc in all_docs:
                        if doc.metadata.get("_source_file") == fn or doc.metadata.get("file_name") == fn:
                            pid = doc.metadata.get("parent_id")
                            if pid and pid not in seen_pid:
                                parent_ids.append(pid)
                                seen_pid.add(pid)
                                break
            
            for doc in all_docs:
                if len(parent_ids) >= top_parents:
                    break
                pid = doc.metadata.get("parent_id")
                if pid and pid not in seen_pid:
                    parent_ids.append(pid)
                    seen_pid.add(pid)
            
            # Chunk í™•ì¥
            expanded_chunks = []
            for pid in parent_ids:
                try:
                    got = vectorstore._collection.get(
                        where={'parent_id': pid},
                        include=['documents', 'metadatas']
                    )
                    chunks = []
                    for txt, meta in zip(got.get("documents", []), got.get("metadatas", [])):
                        if isinstance(meta, dict) and meta.get("doc_type") == "text_chunk":
                            chunks.append((int(meta.get("chunk_index", 0)), txt or "", meta))
                    
                    chunks.sort(key=lambda x: x[0])
                    for _, txt, meta in chunks[:MAX_CHUNKS_PER_PARENT]:
                        expanded_chunks.append(Document(page_content=txt, metadata=meta))
                except:
                    pass
            
            pid_set = set(parent_ids)
            kept_summaries = [d for d in all_docs if d.metadata.get("parent_id") in pid_set]
            final_docs = kept_summaries + expanded_chunks
            
            blocks = []
            for i, d in enumerate(final_docs, start=1):
                m = d.metadata
                text = d.page_content[:MAX_CHARS_PER_DOC]
                blocks.append(f"[{i}] {m.get('file_name', 'unknown')} (p.{m.get('page', '?')})\n{text}")
            
            state["retrieval"] = {
                "docs": final_docs,
                "parent_ids": parent_ids,
                "files_searched": files_searched,
                "doc_count": len(final_docs),
            }
            state["context"] = "\n\n---\n\n".join(blocks)
            return state
            
        except Exception as e:
            state["context"] = ""
            state["retrieval"] = {"docs": [], "parent_ids": [], "files_searched": [], "doc_count": 0}
            return state
    
    # ----- ë…¸ë“œ 6: Rerank & Compress -----
    def rerank_compress(state: GraphState) -> GraphState:
        update_status("ê²°ê³¼ ì •ë ¬ ë° ì••ì¶• ì¤‘...")
        try:
            docs = state["retrieval"].get("docs", [])
            query = state.get("resolved_question", "")
            
            if not docs:
                state["reranked_docs"] = []
                state["compressed_context"] = ""
                return state
            
            query_keywords = set(re.findall(r'[ê°€-í£]+', query))
            
            for doc in docs:
                content_keywords = set(re.findall(r'[ê°€-í£]+', doc.page_content or ""))
                overlap = len(query_keywords & content_keywords)
                doc.metadata["_rerank_score"] = doc.metadata.get("_final_score", 0) + (overlap * 0.01)
            
            docs.sort(key=lambda d: d.metadata.get("_rerank_score", 0), reverse=True)
            
            # ì¤‘ë³µ ì œê±°
            seen_content = set()
            unique_docs = []
            for doc in docs:
                content_hash = hash(doc.page_content[:500])
                if content_hash not in seen_content:
                    seen_content.add(content_hash)
                    unique_docs.append(doc)
            
            compressed_docs = unique_docs[:20]
            
            blocks = []
            for i, d in enumerate(compressed_docs, start=1):
                m = d.metadata
                text = d.page_content[:MAX_CHARS_PER_DOC]
                blocks.append(f"[{i}] {m.get('file_name', 'unknown')} (p.{m.get('page', '?')})\n{text}")
            
            state["reranked_docs"] = compressed_docs
            state["compressed_context"] = "\n\n---\n\n".join(blocks)
            return state
            
        except Exception as e:
            state["reranked_docs"] = state["retrieval"].get("docs", [])
            state["compressed_context"] = state.get("context", "")
            return state
    
    # ----- ë…¸ë“œ 7: Context Sanitize -----
    def context_sanitize(state: GraphState) -> GraphState:
        update_status("ì»¨í…ìŠ¤íŠ¸ ê²€ì¦ ì¤‘...")
        try:
            context = state.get("compressed_context") or state.get("context", "")
            
            danger_patterns = [
                r"(?i)ignore\s+(previous|above|all)\s+instructions?",
                r"(?i)you\s+are\s+now\s+",
                r"(?i)act\s+as\s+",
                r"(?i)system\s*:\s*",
            ]
            
            sanitized = context
            for pattern in danger_patterns:
                sanitized = re.sub(pattern, "[FILTERED]", sanitized)
            
            state["sanitized_context"] = sanitized
            return state
            
        except Exception as e:
            state["sanitized_context"] = state.get("compressed_context") or state.get("context", "")
            return state
    
    # ----- ë…¸ë“œ 8: ë‹µë³€ ìƒì„± -----
    def generate_answer(state: GraphState) -> GraphState:
        retry_count = state.get("retry_count", 0)
        retry_info = f"ì¬ìƒì„± #{retry_count}" if retry_count > 0 and state.get("retry_type") == "generate" else ""
        update_status("ë‹µë³€ ìƒì„± ì¤‘...", retry_info)
        
        try:
            context = state.get("sanitized_context") or state.get("compressed_context") or state.get("context", "")
            
            if not context.strip():
                state["draft_answer"] = "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?"
                return state
            
            if retry_count > 0 and state.get("retry_type") == "generate":
                previous_issue = state.get("validation_reason", "í˜•ì‹ ë¬¸ì œ")
                answer = (get_answer_retry_prompt() | llms["main"] | StrOutputParser()).invoke({
                    "input": state["resolved_question"] or state["input"],
                    "context": context,
                    "previous_issue": previous_issue,
                })
            else:
                answer = (get_answer_prompt() | llms["main"] | StrOutputParser()).invoke({
                    "input": state["resolved_question"] or state["input"],
                    "context": context
                })
            
            state["draft_answer"] = answer
            return state
            
        except Exception as e:
            state["draft_answer"] = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"
            return state
    
    # ----- ë…¸ë“œ 9: Safety Check -----
    def safety_check(state: GraphState) -> GraphState:
        update_status("ì•ˆì „ì„± ê²€ì‚¬ ì¤‘...")
        try:
            answer = state.get("draft_answer", "")
            issues = []
            
            sensitive_patterns = [
                (r"(?i)(ìì‚´|ìí•´)", "ìí•´ ê´€ë ¨ ë‚´ìš©"),
                (r"(?i)(í­ë ¥|í•™ëŒ€)", "í­ë ¥ ê´€ë ¨ ë‚´ìš©"),
            ]
            
            for pattern, issue_name in sensitive_patterns:
                if re.search(pattern, answer):
                    issues.append(issue_name)
            
            state["safety_passed"] = len(issues) == 0
            state["safety_issues"] = issues
            return state
            
        except Exception as e:
            state["safety_passed"] = True
            state["safety_issues"] = []
            return state
    
    # ----- ë…¸ë“œ 10: Validate -----
    def validate_answer(state: GraphState) -> GraphState:
        update_status("ë‹µë³€ ê²€ì¦ ì¤‘...")
        try:
            retry_count = state.get("retry_count", 0)
            
            if retry_count >= MAX_RETRY_COUNT:
                state["validation_result"] = "PASS"
                final_answer = state["draft_answer"]
                
                # âœ… v5.1: ê¸°ë³¸ ì—°ë„ ì‚¬ìš© ì‹œ í™•ì¸ ë©”ì‹œì§€ ì¶”ê°€
                if state.get("used_default_years"):
                    final_answer = _append_year_confirmation(final_answer, state)
                
                state["final_answer"] = final_answer
                return state
            
            context = state.get("sanitized_context") or state.get("context", "")
            
            result = (get_validator_prompt() | llms["main"] | StrOutputParser()).invoke({
                "input": state["resolved_question"] or state["input"],
                "context": context[:15000],
                "answer": state["draft_answer"]
            })
            
            json_match = re.search(r'\{[\s\S]*\}', result)
            if json_match:
                result = json_match.group()
            
            validator_out = json.loads(result)
            state["validator_output"] = validator_out
            
            validation_result = validator_out.get("result", "PASS").upper()
            valid_results = ["PASS", "FAIL_NO_EVIDENCE", "FAIL_UNCLEAR", "FAIL_FORMAT"]
            if validation_result not in valid_results:
                validation_result = "PASS"
            
            state["validation_result"] = validation_result
            state["validation_reason"] = validator_out.get("reason", "")
            
            if validation_result == "PASS":
                corrected = validator_out.get("corrected_answer", "")
                final_answer = corrected if corrected and len(corrected) > 50 else state["draft_answer"]
                
                # âœ… v5.1: ê¸°ë³¸ ì—°ë„ ì‚¬ìš© ì‹œ í™•ì¸ ë©”ì‹œì§€ ì¶”ê°€
                if state.get("used_default_years"):
                    final_answer = _append_year_confirmation(final_answer, state)
                
                state["final_answer"] = final_answer
            elif validation_result == "FAIL_UNCLEAR":
                clarify_q = validator_out.get("clarify_question", "")
                if clarify_q:
                    state["pending_clarification"] = clarify_q
            
            return state
            
        except Exception as e:
            state["validation_result"] = "PASS"
            final_answer = state["draft_answer"]
            
            # âœ… v5.1: ê¸°ë³¸ ì—°ë„ ì‚¬ìš© ì‹œ í™•ì¸ ë©”ì‹œì§€ ì¶”ê°€
            if state.get("used_default_years"):
                final_answer = _append_year_confirmation(final_answer, state)
            
            state["final_answer"] = final_answer
            return state
    
    # âœ… v5.1: ê¸°ë³¸ ì—°ë„ í™•ì¸ ë©”ì‹œì§€ í—¬í¼ í•¨ìˆ˜
    def _append_year_confirmation(answer: str, state: GraphState) -> str:
        years = state.get("plan", {}).get("years", [2023, 2024])
        year_str = ", ".join([f"{y}ë…„" for y in years])
        
        confirmation_msg = (
            f"\n\n---\n"
            f"ğŸ“Œ **ì—°ë„ í™•ì¸ ìš”ì²­**: ì§ˆë¬¸ì— íŠ¹ì • ì—°ë„ê°€ ëª…ì‹œë˜ì§€ ì•Šì•„ "
            f"**ìµœê·¼ ë°ì´í„°({year_str})**ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€ë“œë ¸ìŠµë‹ˆë‹¤. "
            f"ë‹¤ë¥¸ ì—°ë„(2020~2024ë…„)ì˜ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”."
        )
        
        return answer + confirmation_msg
    
    # ----- ë…¸ë“œ 11: Clarify -----
    def handle_clarify(state: GraphState) -> GraphState:
        update_status("ëª…í™•í™” ì§ˆë¬¸ ìƒì„± ì¤‘...")
        try:
            clarify_question = state.get("pending_clarification", "")
            if not clarify_question:
                clarify_question = "ì§ˆë¬¸ì„ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì‹œê² ìŠµë‹ˆê¹Œ? ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • ì—°ë„ë‚˜ ëŒ€ìƒ(ì²­ì†Œë…„, ì„±ì¸ ë“±)ì„ ì§€ì •í•´ ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            
            state["clarification_context"] = {
                "original_query": state["input"],
                "partial_plan": state.get("plan"),
            }
            state["final_answer"] = clarify_question
            return state
            
        except Exception as e:
            state["final_answer"] = "ì§ˆë¬¸ì„ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?"
            return state
    
    # ----- ë…¸ë“œ 12: Retrieve Retry -----
    def retrieve_retry(state: GraphState) -> GraphState:
        state["retry_count"] = (state.get("retry_count") or 0) + 1
        state["retry_type"] = "retrieve"
        
        queries = state["plan"].get("queries", [])
        resolved_q = state.get("resolved_question", "")
        
        synonyms = {
    # í•µì‹¬ ì§€í‘œ/ì§‘ë‹¨
    "ê³¼ì˜ì¡´ë¥ ": ["ê³¼ì˜ì¡´ ìœ„í—˜êµ° ë¹„ìœ¨", "ê³¼ì˜ì¡´ìœ„í—˜êµ° ë¹„ìœ¨", "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ìœ„í—˜êµ° ë¹„ìœ¨", "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´"],
    "ê³¼ì˜ì¡´ìœ„í—˜êµ°": ["ê³¼ì˜ì¡´ ìœ„í—˜êµ°", "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ìœ„í—˜êµ°", "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ìœ„í—˜êµ°", "ê³ ìœ„í—˜êµ°+ì ì¬ì ìœ„í—˜êµ°"],
    "ì ì¬ì ìœ„í—˜êµ°": ["ì ì¬ì  ìœ„í—˜êµ°", "ì ì¬ ìœ„í—˜êµ°"],
    "ì¼ë°˜ì‚¬ìš©ìêµ°": ["ì¼ë°˜ ì‚¬ìš©ìêµ°", "ì¼ë°˜êµ°"],

    # ëŒ€ìƒ/ì—°ë ¹
    "ìœ ì•„ë™": ["ì˜ìœ ì•„", "ìœ ì•„", "ì•„ë™", "ì–´ë¦°ì´", "ë§Œ 3~9ì„¸", "ë§Œ3~9ì„¸", "ë§Œ 3âˆ¼9ì„¸"],
    "ì²­ì†Œë…„": ["10ëŒ€", "ì‹­ëŒ€", "ë§Œ 10~19ì„¸", "ë§Œ10~19ì„¸", "ë§Œ 10âˆ¼19ì„¸"],
    "ì„±ì¸": ["ë§Œ 20~59ì„¸", "ë§Œ20~59ì„¸", "20ëŒ€", "30ëŒ€", "40ëŒ€", "50ëŒ€"],
    "60ëŒ€": ["ê³ ë ¹ì¸µ", "ê³ ë ¹ì", "ë§Œ 60~69ì„¸", "ë§Œ60~69ì„¸", "ë§Œ 60âˆ¼69ì„¸"],

        }
        
        expanded_queries = list(queries)
        for original, alternatives in synonyms.items():
            if original in resolved_q:
                for alt in alternatives:
                    new_query = resolved_q.replace(original, alt)
                    if new_query not in expanded_queries:
                        expanded_queries.append(new_query)
        
        state["plan"]["queries"] = expanded_queries[:8]
        state["rewritten_queries"] = expanded_queries[:8]
        return state
    
    # ----- ë…¸ë“œ 13: Generate Retry -----
    def generate_retry(state: GraphState) -> GraphState:
        state["retry_count"] = (state.get("retry_count") or 0) + 1
        state["retry_type"] = "generate"
        return state
    
    return {
        "route_intent": route_intent,
        "smalltalk": handle_smalltalk,
        "offtopic": handle_offtopic,
        "chat_ref": handle_chat_reference,
        "plan_search": plan_search,
        "query_rewrite": query_rewrite,
        "retrieve": retrieve_documents,
        "rerank_compress": rerank_compress,
        "context_sanitize": context_sanitize,
        "generate": generate_answer,
        "safety_check": safety_check,
        "validate": validate_answer,
        "clarify": handle_clarify,
        "retrieve_retry": retrieve_retry,
        "generate_retry": generate_retry,
    }

# =========================================================
# ê·¸ë˜í”„ ë¹Œë”
# =========================================================
def build_graph(node_functions):
    workflow = StateGraph(GraphState)
    
    for name, func in node_functions.items():
        workflow.add_node(name, func)
    
    def route_by_intent(state: GraphState) -> str:
        intent = state.get("intent", "RAG")
        if intent == "SMALLTALK":
            return "smalltalk"
        elif intent == "OFFTOPIC":
            return "offtopic"
        elif intent == "CHAT_REF":
            return "chat_ref"
        else:
            return "rag_pipeline"
    
    def route_after_validate(state: GraphState) -> str:
        retry_count = state.get("retry_count", 0)
        if retry_count >= MAX_RETRY_COUNT:
            return "end"
        
        result = state.get("validation_result", "PASS")
        if result == "PASS":
            return "end"
        elif result == "FAIL_NO_EVIDENCE":
            return "retrieve_retry"
        elif result == "FAIL_UNCLEAR":
            return "clarify"
        elif result == "FAIL_FORMAT":
            return "generate_retry"
        return "end"
    
    workflow.set_entry_point("route_intent")
    
    workflow.add_conditional_edges(
        "route_intent",
        route_by_intent,
        {
            "smalltalk": "smalltalk",
            "offtopic": "offtopic",
            "chat_ref": "chat_ref",
            "rag_pipeline": "plan_search"
        }
    )
    
    workflow.add_edge("smalltalk", END)
    workflow.add_edge("offtopic", END)
    workflow.add_edge("chat_ref", END)
    workflow.add_edge("clarify", END)
    
    workflow.add_edge("plan_search", "query_rewrite")
    workflow.add_edge("query_rewrite", "retrieve")
    workflow.add_edge("retrieve", "rerank_compress")
    workflow.add_edge("rerank_compress", "context_sanitize")
    workflow.add_edge("context_sanitize", "generate")
    workflow.add_edge("generate", "safety_check")
    workflow.add_edge("safety_check", "validate")
    
    workflow.add_conditional_edges(
        "validate",
        route_after_validate,
        {
            "end": END,
            "retrieve_retry": "retrieve_retry",
            "clarify": "clarify",
            "generate_retry": "generate_retry"
        }
    )
    
    workflow.add_edge("retrieve_retry", "retrieve")
    workflow.add_edge("generate_retry", "generate")
    
    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)

# =========================================================
# ë©”ì¸ UI
# =========================================================
def main():
    st.title("ğŸ“Š ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë¶„ì„ ì‹œìŠ¤í…œ v5")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ“‹ ì‹œìŠ¤í…œ ì •ë³´")
        st.markdown(BOT_IDENTITY)
        
        st.divider()
        
        st.subheader("ğŸ”§ v5 ìƒˆ ê¸°ëŠ¥")
        st.caption("âœ… íšŒë³µ ë£¨í”„ (ê²€ìƒ‰/ìƒì„± ì¬ì‹œë„)")
        st.caption("âœ… Query Rewrite (ì¿¼ë¦¬ ìµœì í™”)")
        st.caption("âœ… Rerank & Compress")
        st.caption("âœ… Safety Guard")
        
        st.divider()
        
        if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.messages = []
            st.session_state.chat_history = []
            st.session_state.clarification_context = None
            st.rerun()
        
        st.divider()
        
        debug_mode = st.checkbox("ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ", value=False)
    
    # ì‚¬ìš©ì ê°€ì´ë“œ ë°•ìŠ¤
    st.markdown("""
    <div class="guide-box">
        <div class="guide-title">ğŸ“Œ ì‚¬ìš© ì•ˆë‚´</div>
        <div class="guide-item">
            <strong>â„¹ï¸ ìš©ë„:</strong> ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ(2020~2024) <strong>ë‹¨ìˆœ ì •ë³´ ê²€ìƒ‰ìš©</strong>ì…ë‹ˆë‹¤. <br>
            ì¸ì‚¬ì´íŠ¸ ì œê³µ, ì¼ë°˜ ëŒ€í™”, ë³´ê³ ì„œ ì™¸ ì •ë³´ ê²€ìƒ‰ì—ëŠ” ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        </div>
        <div class="guide-item">
            <strong>ğŸ’¡ ê²€ìƒ‰ íŒ:</strong> ì§ˆë¬¸ì€ <strong>ìµœëŒ€í•œ êµ¬ì²´ì ìœ¼ë¡œ</strong> ì‘ì„±í•´ ì£¼ì„¸ìš”.<br>
            ê³¼ë„í•œ ê²€ìƒ‰ê²°ê³¼ ë°©ì§€ë¥¼ ìœ„í•œ ì„¤ì •ìœ¼ë¡œ ì¸í•´ ì¼ë¶€ ì—°ë„ê°€ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ëˆ„ë½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ´ ë•ŒëŠ” í•´ë‹¹ ì—°ë„ë¥¼ ì§€ì •í•´ì„œ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.<br>
            ë³´ê³ ì„œ ë‚´ ìœ ì‚¬í•œ ë‚´ìš©ì´ ë‹¤ìˆ˜ ìˆì–´, ê²€ìƒ‰ ì„±ëŠ¥ì´ ì•ˆë‚˜ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš”êµ¬í•˜ê³ ìí•˜ëŠ” ë°”ë¥¼ í™•ì‹¤íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”<br>
            ì˜ˆ) "ê³¼ì˜ì¡´ë¥ " â†’ "2024ë…„ ì²­ì†Œë…„ ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ìœ„í—˜êµ° ë¹„ìœ¨"
            ì˜ˆ) "ìˆí¼ê³¼ ê³¼ì˜ì¡´" â†’ "ìˆí¼ ì´ìš©ë¥ ì— ë”°ë¥¸ ê³¼ì˜ì¡´ ì°¨ì´" or "ê³¼ì˜ì¡´ìœ„í—˜êµ°ë³„ ìˆí¼ ì´ìš© íŠ¹ì„±ì˜ ì°¨ì´"
        </div>
        <div class="guide-item">
            <strong>âš ï¸ ì£¼ì˜:</strong> AI ë‹µë³€ì— <strong>ì˜¤ë¥˜(í• ë£¨ì‹œë„¤ì´ì…˜)</strong>ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. <br>
            ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”ë¡œ ì¸ìš©í•˜ì§€ ë§ˆì‹œê³ , <strong>ì›ë¬¸ì„ í†µí•´ í•œë²ˆ ë” í™•ì¸í•œ ë’¤</strong> ì •ë³´ë¥¼ ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì‹­ì‹œìš”.
            <a href="https://www.nia.or.kr" target="_blank" style="color: #fff;">NIA í™ˆí˜ì´ì§€</a>ì—ì„œ ì›ë¬¸ í™•ì¸ ê¶Œì¥.
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # DB ë‹¤ìš´ë¡œë“œ
    if not os.path.exists(LOCAL_DB_PATH) or not os.listdir(LOCAL_DB_PATH):
        st.info("ğŸ”„ Chroma DBë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        with st.spinner(f"Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘... ({HF_REPO_ID})"):
            db_path, error = download_chroma_db()
        
        if error:
            st.error(f"DB ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {error}")
            return
        else:
            st.success("DB ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            st.rerun()
    
    # ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”
    vectorstore, llms, error = init_resources()
    
    if error:
        st.error(f"ì´ˆê¸°í™” ì˜¤ë¥˜: {error}")
        if "API" in error:
            st.info("Streamlit Secretsì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            with st.form("api_key_form"):
                api_key = st.text_input("OpenAI API í‚¤", type="password")
                if st.form_submit_button("ì„¤ì •") and api_key:
                    os.environ['OPENAI_API_KEY'] = api_key
                    st.rerun()
        return
    
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["role"] == "assistant":
                render_answer_with_tables(message["content"])
            else:
                st.markdown(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”... (ì˜ˆ: 2024ë…„ ì²­ì†Œë…„ ê³¼ì˜ì¡´ë¥ ì€?)"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            status_placeholder = st.empty()
            answer_placeholder = st.empty()
            
            try:
                node_functions = create_node_functions(vectorstore, llms, status_placeholder)
                graph = build_graph(node_functions)
                
                config = {"configurable": {"thread_id": "streamlit_session"}}
                
                result = graph.invoke(
                    {
                        "input": prompt,
                        "chat_history": st.session_state.chat_history,
                        "session_id": "streamlit_session",
                        "clarification_context": st.session_state.clarification_context,
                    },
                    config=config
                )
                
                status_placeholder.empty()
                
                # Clarification context ì €ì¥
                if result.get("clarification_context"):
                    st.session_state.clarification_context = result["clarification_context"]
                else:
                    st.session_state.clarification_context = None
                
                final_answer = result.get("final_answer", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                
                with answer_placeholder.container():
                    render_answer_with_tables(final_answer)
                
                # ë””ë²„ê·¸ ì •ë³´
                if debug_mode:
                    with st.expander("ğŸ” ë””ë²„ê·¸ ì •ë³´ (v5)", expanded=False):
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.write(f"**Intent:** {result.get('intent', 'N/A')}")
                            st.write(f"**Followup:** {result.get('followup_type', 'N/A')}")
                            st.write(f"**Retry Count:** {result.get('retry_count', 0)}")
                            st.write(f"**Default Years Used:** {result.get('used_default_years', False)}")
                            
                            validation_result = result.get('validation_result', 'N/A')
                            if validation_result == "PASS":
                                st.markdown(f"**Validation:** <span class='validation-pass'>{validation_result}</span>", unsafe_allow_html=True)
                            else:
                                st.markdown(f"**Validation:** <span class='validation-fail'>{validation_result}</span>", unsafe_allow_html=True)
                        
                        with col2:
                            if result.get("rewritten_queries"):
                                st.write("**Rewritten Queries:**")
                                for q in result["rewritten_queries"][:3]:
                                    st.caption(f"â€¢ {q[:50]}...")
                        
                        if result.get("retrieval"):
                            st.write(f"**ê²€ìƒ‰ íŒŒì¼:** {result['retrieval'].get('files_searched', [])}")
                            st.write(f"**ë¬¸ì„œ ìˆ˜:** {result['retrieval'].get('doc_count', 0)}")
                        
                        if result.get("plan"):
                            st.write(f"**ê²€ìƒ‰ ì—°ë„:** {result['plan'].get('years', [])}")
                        
                        if result.get("validation_reason"):
                            st.write(f"**Validation Reason:** {result['validation_reason'][:100]}")
                        
                        st.write(f"**Safety:** passed={result.get('safety_passed', 'N/A')}")
                
                st.session_state.messages.append({"role": "assistant", "content": final_answer})
                st.session_state.chat_history.append(HumanMessage(content=prompt))
                st.session_state.chat_history.append(AIMessage(content=final_answer))
                
                if len(st.session_state.chat_history) > 20:
                    st.session_state.chat_history = st.session_state.chat_history[-20:]
                
            except Exception as e:
                status_placeholder.empty()
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                if debug_mode:
                    import traceback
                    st.code(traceback.format_exc())

if __name__ == "__main__":
    main()


